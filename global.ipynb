{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Tuple\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import models, transforms\n",
    "from torchvision.utils import save_image, make_grid\n",
    "import numpy as np\n",
    "from diffusion_utilities import *\n",
    "from PIL import Image\n",
    "\n",
    "class ContextUnet(nn.Module):\n",
    "    def __init__(self, in_channels, n_feat=256, n_cfeat=10, height=28):  # cfeat - context features\n",
    "        super(ContextUnet, self).__init__()\n",
    "\n",
    "        # number of input channels, number of intermediate feature maps and number of classes\n",
    "        self.in_channels = in_channels\n",
    "        self.n_feat = n_feat\n",
    "        self.n_cfeat = n_cfeat\n",
    "        self.h = height  #assume h == w. must be divisible by 4, so 28,24,20,16...\n",
    "\n",
    "        # Initialize the initial convolutional layer\n",
    "        self.init_conv = ResidualConvBlock(in_channels, n_feat, is_res=True)\n",
    "\n",
    "        # Initialize the down-sampling path of the U-Net with two levels\n",
    "        self.down1 = UnetDown(n_feat, n_feat)        # down1 #[10, 256, 8, 8]\n",
    "        self.down2 = UnetDown(n_feat, 2 * n_feat)    # down2 #[10, 256, 4,  4]\n",
    "        \n",
    "         # original: self.to_vec = nn.Sequential(nn.AvgPool2d(7), nn.GELU())\n",
    "        self.to_vec = nn.Sequential(nn.AvgPool2d((4)), nn.GELU())\n",
    "\n",
    "        # Embed the timestep and context labels with a one-layer fully connected neural network\n",
    "        self.timeembed1 = EmbedFC(1, 2*n_feat)\n",
    "        self.timeembed2 = EmbedFC(1, 1*n_feat)\n",
    "        self.contextembed1 = EmbedFC(n_cfeat, 2*n_feat)\n",
    "        self.contextembed2 = EmbedFC(n_cfeat, 1*n_feat)\n",
    "\n",
    "        # Initialize the up-sampling path of the U-Net with three levels\n",
    "        self.up0 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(2 * n_feat, 2 * n_feat, self.h//4, self.h//4), # up-sample  \n",
    "            nn.GroupNorm(8, 2 * n_feat), # normalize                       \n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.up1 = UnetUp(4 * n_feat, n_feat)\n",
    "        self.up2 = UnetUp(2 * n_feat, n_feat)\n",
    "\n",
    "        # Initialize the final convolutional layers to map to the same number of channels as the input image\n",
    "        self.out = nn.Sequential(\n",
    "            nn.Conv2d(2 * n_feat, n_feat, 3, 1, 1), # reduce number of feature maps   #in_channels, out_channels, kernel_size, stride=1, padding=0\n",
    "            nn.GroupNorm(8, n_feat), # normalize\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(n_feat, self.in_channels, 3, 1, 1), # map to same number of channels as input\n",
    "        )\n",
    "\n",
    "    def forward(self, x, t, c=None):\n",
    "        \"\"\"\n",
    "        x : (batch, n_feat, h, w) : input image\n",
    "        t : (batch, n_cfeat)      : time step\n",
    "        c : (batch, n_classes)    : context label\n",
    "        \"\"\"\n",
    "        # x is the input image, c is the context label, t is the timestep, context_mask says which samples to block the context on\n",
    "\n",
    "        # pass the input image through the initial convolutional layer\n",
    "        x = self.init_conv(x)\n",
    "        # pass the result through the down-sampling path\n",
    "        down1 = self.down1(x)       #[10, 256, 8, 8]\n",
    "        down2 = self.down2(down1)   #[10, 256, 4, 4]\n",
    "        \n",
    "        # convert the feature maps to a vector and apply an activation\n",
    "        hiddenvec = self.to_vec(down2)\n",
    "        \n",
    "        # mask out context if context_mask == 1\n",
    "        if c is None:\n",
    "            c = torch.zeros(x.shape[0], self.n_cfeat).to(x)\n",
    "            \n",
    "        # embed context and timestep\n",
    "        cemb1 = self.contextembed1(c).view(-1, self.n_feat * 2, 1, 1)     # (batch, 2*n_feat, 1,1)\n",
    "        temb1 = self.timeembed1(t).view(-1, self.n_feat * 2, 1, 1)\n",
    "        cemb2 = self.contextembed2(c).view(-1, self.n_feat, 1, 1)\n",
    "        temb2 = self.timeembed2(t).view(-1, self.n_feat, 1, 1)\n",
    "        #print(f\"uunet forward: cemb1 {cemb1.shape}. temb1 {temb1.shape}, cemb2 {cemb2.shape}. temb2 {temb2.shape}\")\n",
    "\n",
    "\n",
    "        up1 = self.up0(hiddenvec)\n",
    "        up2 = self.up1(cemb1*up1 + temb1, down2)  # add and multiply embeddings\n",
    "        up3 = self.up2(cemb2*up2 + temb2, down1)\n",
    "        out = self.out(torch.cat((up3, x), 1))\n",
    "        return out\n",
    "\n",
    "class Noise:\n",
    "    def __init__(self, timesteps, device):\n",
    "        # diffusion hyperparameters    \n",
    "        self.beta1 = 1e-4\n",
    "        self.beta2 = 0.02\n",
    "\n",
    "        # construct DDPM noise schedule\n",
    "        self.b_t = (self.beta2 - self.beta1) * torch.linspace(0, 1, timesteps + 1, device=device) + self.beta1\n",
    "        self.a_t = 1 - self.b_t\n",
    "        self.ab_t = torch.cumsum(self.a_t.log(), dim=0).exp()    \n",
    "        self.ab_t[0] = 1\n",
    "\n",
    "    # helper function; removes the predicted noise (but adds some noise back in to avoid collapse)\n",
    "    def denoise_add_noise(self, x, t, pred_noise, z=None):\n",
    "        if z is None:\n",
    "            z = torch.randn_like(x)\n",
    "        noise = self.b_t.sqrt()[t] * z\n",
    "        mean = (x - pred_noise * ((1 - self.a_t[t]) / (1 - self.ab_t[t]).sqrt())) / self.a_t[t].sqrt()\n",
    "        return mean + noise\n",
    "\n",
    "# sample using standard algorithm\n",
    "@torch.no_grad()\n",
    "def sample_ddpm(n_sample, height, timesteps, noise, nn_model, device, save_rate=20):\n",
    "    # x_T ~ N(0, 1), sample initial noise\n",
    "    samples = torch.randn(n_sample, 3, height, height).to(device)  \n",
    "\n",
    "    # array to keep track of generated steps for plotting\n",
    "    intermediate = [] \n",
    "    for i in range(timesteps, 0, -1):\n",
    "        print(f'sampling timestep {i:3d}', end='\\r')\n",
    "\n",
    "        # reshape time tensor\n",
    "        t = torch.tensor([i / timesteps])[:, None, None, None].to(device)\n",
    "\n",
    "        # sample some random noise to inject back in. For i = 1, don't add back in noise\n",
    "        z = torch.randn_like(samples) if i > 1 else 0\n",
    "\n",
    "        eps = nn_model(samples, t)    # predict noise e_(x_t,t)\n",
    "        samples = noise.denoise_add_noise(samples, i, eps, z)\n",
    "        if i % save_rate ==0 or i==timesteps or i<8:\n",
    "            intermediate.append(samples.detach().cpu().numpy())\n",
    "\n",
    "    intermediate = np.stack(intermediate)\n",
    "    return samples, intermediate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "construct model\n",
      "Loaded in Model\n"
     ]
    }
   ],
   "source": [
    "# diffusion hyperparameters\n",
    "timesteps = 600\n",
    "\n",
    "# network hyperparameters\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else torch.device('cpu'))\n",
    "n_feat = 64 # 64 hidden dimension feature\n",
    "n_cfeat = 5 # context vector is of size 5\n",
    "height = 16 # 16x16 image\n",
    "save_dir = './weights/'\n",
    "\n",
    "# construct model\n",
    "print(\"construct model\")\n",
    "nn_model = ContextUnet(in_channels=3, n_feat=n_feat, n_cfeat=n_cfeat, height=height).to(device)\n",
    "\n",
    "# load in model weights and set to eval mode\n",
    "nn_model.load_state_dict(torch.load(f\"{save_dir}/model_trained.pth\", map_location=device))\n",
    "nn_model.eval()\n",
    "print(\"Loaded in Model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise = Noise(timesteps, device)\n",
    "\n",
    "n_sample = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sampling timestep 599\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sampling timestep   1\r"
     ]
    }
   ],
   "source": [
    "samples, intermediate = sample_ddpm(n_sample=n_sample, height=height, timesteps=timesteps, noise=noise, nn_model=nn_model, device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i in range(n_sample):\n",
    "    pil_image = transforms.ToPILImage()(samples[i])\n",
    "    plt.subplot(4, 8, i+1)\n",
    "    plt.imshow(pil_image)\n",
    "    plt.axis('off')\n",
    "    plt.subplots_adjust(wspace=0, hspace=-0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 2 is out of bounds for axis 1 with size 2",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[45], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m plt\u001b[38;5;241m.\u001b[39mclf()\n\u001b[1;32m----> 2\u001b[0m animation_ddpm \u001b[38;5;241m=\u001b[39m \u001b[43mplot_sample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mintermediate\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43msave_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mani_run\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m HTML(animation_ddpm\u001b[38;5;241m.\u001b[39mto_jshtml())\n",
      "File \u001b[1;32mc:\\joel\\dev\\ai\\How-Diffusion-Models-Work\\diffusion_utilities.py:194\u001b[0m, in \u001b[0;36mplot_sample\u001b[1;34m(x_gen_store, n_sample, nrows, save_dir, fn, w, save)\u001b[0m\n\u001b[0;32m    192\u001b[0m ncols \u001b[38;5;241m=\u001b[39m n_sample\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39mnrows\n\u001b[0;32m    193\u001b[0m sx_gen_store \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmoveaxis(x_gen_store,\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m4\u001b[39m)                               \u001b[38;5;66;03m# change to Numpy image format (h,w,channels) vs (channels,h,w)\u001b[39;00m\n\u001b[1;32m--> 194\u001b[0m nsx_gen_store \u001b[38;5;241m=\u001b[39m \u001b[43mnorm_all\u001b[49m\u001b[43m(\u001b[49m\u001b[43msx_gen_store\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msx_gen_store\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_sample\u001b[49m\u001b[43m)\u001b[49m   \u001b[38;5;66;03m# unity norm to put in range [0,1] for np.imshow\u001b[39;00m\n\u001b[0;32m    196\u001b[0m \u001b[38;5;66;03m# create gif of images evolving over time, based on x_gen_store\u001b[39;00m\n\u001b[0;32m    197\u001b[0m fig, axs \u001b[38;5;241m=\u001b[39m plt\u001b[38;5;241m.\u001b[39msubplots(nrows\u001b[38;5;241m=\u001b[39mnrows, ncols\u001b[38;5;241m=\u001b[39mncols, sharex\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, sharey\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,figsize\u001b[38;5;241m=\u001b[39m(ncols,nrows))\n",
      "File \u001b[1;32mc:\\joel\\dev\\ai\\How-Diffusion-Models-Work\\diffusion_utilities.py:155\u001b[0m, in \u001b[0;36mnorm_all\u001b[1;34m(store, n_t, n_s)\u001b[0m\n\u001b[0;32m    153\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_t):\n\u001b[0;32m    154\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_s):\n\u001b[1;32m--> 155\u001b[0m         nstore[t,s] \u001b[38;5;241m=\u001b[39m unorm(\u001b[43mstore\u001b[49m\u001b[43m[\u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43ms\u001b[49m\u001b[43m]\u001b[49m)\n\u001b[0;32m    156\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m nstore\n",
      "\u001b[1;31mIndexError\u001b[0m: index 2 is out of bounds for axis 1 with size 2"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.clf()\n",
    "animation_ddpm = plot_sample(intermediate,5,1,save_dir, \"ani_run\", None, save=False)\n",
    "HTML(animation_ddpm.to_jshtml())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
